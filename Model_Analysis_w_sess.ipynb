{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis w session OHE info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.insert(0,\"/Library/Python/2.7/site-packages/\") \n",
    "#sys.path.insert(0,\"/Library/Frameworks/Python.framework/Versions/2.7/bin\")\n",
    "%load_ext autoreload\n",
    "%autoreload 1 # autoreload all modules imported w %aimport automatically before code execution\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/df_all_inc_sess.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get X and y\n",
    "X = data[0:213466]\n",
    "y = pd.read_csv(\"./data/y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop last 15 samples as they're caused by a bug (https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings/forums/t/17737/announcement-competition-will-restart-shortly)\n",
    "X = X[0:213451]\n",
    "y = y[0:213451]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take only samples w session info\n",
    "X = X[X.hit_count > 0]\n",
    "y = y.ix[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop user_id\n",
    "X = X.drop(['user_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for users w/out session info, set relevant features to -1.\n",
    "X.fillna(value=-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73815, 770)\n",
      "(73815,)\n"
     ]
    }
   ],
   "source": [
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is indexed by dict:\n",
      "{'FR': 4, 'NL': 10, 'PT': 11, 'CA': 6, 'DE': 8, 'IT': 5, 'US': 3, 'NDF': 1, 'other': 0, 'AU': 7, 'GB': 2, 'ES': 9}\n"
     ]
    }
   ],
   "source": [
    "# encode y values\n",
    "y_labels = {}\n",
    "i = 0\n",
    "for value in y.country_destination.unique():\n",
    "    if value not in y_labels.keys():\n",
    "        y_labels[value] = i\n",
    "        i += 1\n",
    "    y[y == value] = i-1\n",
    "\n",
    "print \"y is indexed by dict:\\n%s\" % y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set y to array-like type\n",
    "y = y.country_destination\n",
    "y.ravel().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51670, 770)\n",
      "(51670,)\n",
      "(22145, 770)\n",
      "(22145,)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.to_csv(\"./data/temp/X_train.csv\", index=False)\n",
    "y_train.to_csv(\"./data/temp/y_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test.to_csv(\"./data/temp/X_test.csv\", index=False)\n",
    "y_test.to_csv(\"./data/temp/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_100 = X_train[0:100]\n",
    "y_100 = y_train[0:100]\n",
    "X_100.to_csv(\"./data/temp/X_100.csv\", index=False)\n",
    "y_100.to_csv(\"./data/temp/y_100.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evio.evolver.ConceptEvolutionClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/Users/gokhan/libs\")\n",
    "%aimport from Evio.evolver import ConceptEvolutionClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cec = ConceptEvolutionClassifier(n_concepts=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cec.p_mut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cec.fit(X_train[0:100], y_train[0:100], 12, n_folds=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost accuracy 0.6371 (n_estimators=200) (NA values filled with -1)\n",
    "## n_estimators=100 => acc. 0.6352 (NA values filled with -1)\n",
    "## n_estimators=300 => acc. 0.6357 (NA values filled with -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "abo = AdaBoostClassifier(n_estimators=300)\n",
    "abo.fit(X_train, y_train.astype(int))\n",
    "y_pred_abo = abo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print metrics.accuracy_score(y_test.astype(int), y_pred_abo.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_matrix_abo = metrics.confusion_matrix(y_test.astype(int), y_pred_abo.astype(int))\n",
    "print conf_matrix_abo\n",
    "print sum(sum(conf_matrix_abo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_proba_test_abo = abo.predict_proba(X_test)\n",
    "predict_proba_test_abo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_proba_test_abo[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_importances = []\n",
    "for i in range(len(X_test.columns)):\n",
    "    #print \"%s: %.3f\" % (X_test.columns[i], abo.feature_importances_[i])\n",
    "    feat_importances.append((X_test.columns[i], abo.feature_importances_[i]))\n",
    "\n",
    "feat_importances = sorted(feat_importances, key=lambda x: x[1], reverse=True)\n",
    "for pair in feat_importances:\n",
    "    print pair[0], pair[1]\n",
    "\n",
    "\n",
    "#print X_train.columns\n",
    "#print abo.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes Classifier\n",
    "## Accuracy 0.5655"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_gnb = gnb.fit(X_train, y_train.astype(int)).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print metrics.accuracy_score(y_test.astype(int), y_pred_gnb.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_matrix_gnb = metrics.confusion_matrix(y_test.astype(int), y_pred_gnb.astype(int))\n",
    "print conf_matrix_gnb\n",
    "print sum(sum(conf_matrix_gnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree Classifier\n",
    "## Acc: 0.5250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train.astype(int))\n",
    "y_pred_dtc = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print metrics.accuracy_score(y_test.astype(int), y_pred_dtc.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_matrix_dtc = metrics.confusion_matrix(y_test.astype(int), y_pred_dtc.astype(int))\n",
    "print conf_matrix_dtc\n",
    "print sum(sum(conf_matrix_dtc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest Classifier\n",
    "## n_estimators=10   ; Acc= 0.6054\n",
    "## n_estimators=100 ; Acc= 0.6201\n",
    "## n_estimators=300 ; Acc= 0.6214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=300, n_jobs=4, verbose=True)\n",
    "rfc = rfc.fit(X_train, y_train.astype(int))\n",
    "y_pred_rfc = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print metrics.accuracy_score(y_test.astype(int), y_pred_rfc.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_matrix_rfc = metrics.confusion_matrix(y_test.astype(int), y_pred_rfc.astype(int))\n",
    "print conf_matrix_rfc\n",
    "print sum(sum(conf_matrix_rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eXtremeGradientBoosting\n",
    "## Acc=0.6383 (max_depth=2, eta=1, multi:softmax, ) => w a eval-merror: ~0.361665\n",
    "## Acc=0.42 !!!! (max_depth=2, eta=1, multi:softmax, missing=-1) => w a eval-meror: ~0.361430\n",
    "## Acc=0.5755 (max_depth=4, eta=1, multi:softmax, missing=-1) => w a eval-meror: ~0.354794\n",
    "## Acc=0.58 (max_depth=4, eta=1, multi:softmax, missing=-1) => w a eval-meror: ~0.352108\n",
    "## Acc=0.6271 (max_depth=4, eta=1, multi:softmax, missing=-1) => w a eval-meror: ~0.358729\n",
    "## Acc=0.6273(max_depth=6, eta=0.1, multi:softmax, missing=-1) => w a(eval-merror:0.350515\ttrain-merror:0.346258)\n",
    "## Acc=0.6299(max_depth=6, eta=0.1, multi:softmax, missing=-1) => w ([16]\teval-merror:0.351156\ttrain-merror:0.347570)\n",
    "## Acc=0.6309(max_depth=6, eta=0.1, multi:softmax, missing=-1) => w ([1]\teval-merror:0.354700\ttrain-merror:0.353419)\n",
    "## Acc=0.6272(max_depth=6, eta=0.1, multi:softmax, missing=-1, eval_metric=auc) => w ([24]\teval-auc:0.530073\ttrain-auc:0.498108)\n",
    "## Acc=0.6281(max_depth=6, eta=0.1, multi:softmax, missing=-1, eval_metric=auc) => w ([19]\teval-auc:0.530904\ttrain-auc:0.497756)\n",
    "## Acc=0.6284(max_depth=6, eta=0.1, multi:softmax, missing=-1, eval_metric=auc) => w ([20]\teval-auc:0.530723\ttrain-auc:0.498060)\n",
    "\n",
    "## Acc=0.6170 (max_depth=2, eta=0.1, multi:softmax, eval_metric=auc) => w ([24]\teval-auc:0.530798\ttrain-auc:0.498628)\n",
    "## Acc=0.6366 (max_depth=2, eta=0.1, multi:softmax, eval_metric=auc) => w ([10]\teval-auc:0.530994\ttrain-auc:0.499466)\n",
    "## Acc=0.6388 (max_depth=2, eta=0.1, multi:softmax, eval_metric=auc) => w ([5]\teval-auc:0.530022\ttrain-auc:0.500733)\n",
    "## Acc=0.6391 (max_depth=2, eta=0.1, multi:softmax, eval_metric=auc) => w ([7]\teval-auc:0.531386\ttrain-auc:0.500006)\n",
    "## Acc=0.6253 (max_depth=2, eta=0.1, multi:softmax, eval_metric=auc) => w ([49]\teval-auc:0.531864\ttrain-auc:0.500586)\n",
    "## Acc=0.6246 (max_depth=2, eta=0.1, multi:softmax, eval_metric=auc) => w ([48]\teval-auc:0.531936\ttrain-auc:0.500466)\n",
    "## Acc=0.6208 (max_depth=2, eta=0.1, multi:softmax, eval_metric=auc) => w ([45]\teval-auc:0.531965\ttrain-auc:0.500355)\n",
    "\n",
    "## Acc=0.6380 (max_depth=3, eta=0.1, multi:softmax, eval_metric=auc) => w ([49]\teval-auc:0.532793\ttrain-auc:0.500927)\n",
    "## Acc=0.6369 (max_depth=3, eta=0.1, multi:softmax, eval_metric=auc) => w ([42]\teval-auc:0.533173\ttrain-auc:0.500538)\n",
    "## Acc=0.6377 (max_depth=3, eta=0.1, multi:softmax, eval_metric=auc) => w ([48]\teval-auc:0.532941\ttrain-auc:0.500901)\n",
    "## Acc=0.6406 (max_depth=3, eta=0.1, multi:softmax, eval_metric=auc) => w ([99]\teval-auc:0.532941\ttrain-auc:0.500901)\n",
    "## Acc=0.6398 (max_depth=3, eta=0.1, multi:softmax, eval_metric=auc) => w ([50+28]\teval-auc:0.532336\ttrain-auc:0.501438)\n",
    "## Acc=0.6404 (max_depth=3, eta=0.1, multi:softmax, eval_metric=auc) => w ([150] eval-auc:0.531397\ttrain-auc:0.500827)\n",
    "## Acc=0.6403 (max_depth=3, eta=0.1, multi:softmax, eval_metric=auc) => w ([100]\teval-auc:0.532247\ttrain-auc:0.501267)\n",
    "## Acc=0.6404 (max_depth=3, eta=0.1, multi:softmax, eval_metric=auc) => w ([102] eval-auc:0.532046\ttrain-auc:0.501307)\n",
    "\n",
    "## Acc=0.6307 (max_depth=4, eta=0.1, multi:softmax, eval_metric=auc) => w ([199]\teval-auc:0.530604\ttrain-auc:0.500575)\n",
    "## Acc=0.6286 (max_depth=4, eta=0.1, multi:softmax, eval_metric=auc) => w ([0]\teval-auc:0.536492\ttrain-auc:0.499501)\n",
    "## Acc=0.6362 (max_depth=4, eta=0.1, multi:softmax, eval_metric=auc) => w ([30]\teval-auc:0.532703\ttrain-auc:0.499100)\n",
    "## Acc=0.6341 (max_depth=4, eta=0.1, multi:softmax, eval_metric=auc) => w ([42]\teval-auc:0.532502\ttrain-auc:0.500677)\n",
    "## Acc=0.6333 (max_depth=4, eta=0.1, multi:softmax, eval_metric=auc) => w ([86]\teval-auc:0.532818\ttrain-auc:0.501482)\n",
    "## Acc=0.6332 (max_depth=4, eta=0.1, multi:softmax, eval_metric=auc) => w ([140]\teval-auc:0.530992\ttrain-auc:0.501007)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " import xgboost as xgb\n",
    "\n",
    "#param = {'bst:max_depth':2, 'bst:eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "param = {'bst:max_depth':4, 'bst:eta':0.1, 'silent':0, 'objective': 'multi:softmax', 'num_class': 12}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "\n",
    "num_round = 200\n",
    "dtrain = xgb.DMatrix( X_train, label=y_train, missing=-1)\n",
    "dtest = xgb.DMatrix( X_test, label=y_test, missing=-1)\n",
    "evallist  = [(dtest,'eval'), (dtrain,'train')]\n",
    "#bst = xgb.train( param.items(), dtrain, num_round, evallist, early_stopping_rounds=10)\n",
    "bst = xgb.train( param.items(), dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# continue previous training\n",
    "bst = xgb.train( param.items(), dtrain, num_round, evallist, xgb_model=bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print bst.best_ntree_limit\n",
    "print bst.best_iteration\n",
    "#print bst.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We must set ntree_limit=bst.best_ntree_limit, \n",
    "# and prior to that bst.best_ntree_limit must be set to: [best_eval-merror_iteration_index+1]\n",
    "bst.best_ntree_limit = 141\n",
    "bst.best_iteration = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_xgb = bst.predict(xgb.DMatrix(X_test), ntree_limit=bst.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print metrics.accuracy_score(y_test.astype(int), y_pred_xgb.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_matrix_xgb = metrics.confusion_matrix(y_test.astype(int), y_pred_xgb.astype(int))\n",
    "print conf_matrix_xgb\n",
    "print sum(sum(conf_matrix_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=2, weights='distance')\n",
    "knn.fit(X_train, y_train.astype(int))\n",
    "y_pred_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf_matrix_knn = metrics.confusion_matrix(y_test.astype(int), y_pred_knn.astype(int))\n",
    "print conf_matrix_knn\n",
    "\n",
    "print \"Total items: %d\" % sum(sum(conf_matrix_knn))\n",
    "\n",
    "print \"Accuracy: %.4f\" % metrics.accuracy_score(y_test.astype(int), y_pred_knn.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM takes too much time!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(C=1.0, verbose=True)\n",
    "svm.fit(X_train, y_train.astype(int))\n",
    "y_pred_svm = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print metrics.accuracy_score(y_test.astype(int), y_pred_svm.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf_matrix_svm = metrics.confusion_matrix(y_test.astype(int), y_pred_svm.astype(int))\n",
    "print conf_matrix_svm\n",
    "print sum(sum(conf_matrix_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoosting (n_estimators=100, max_depth=3, learning_rate=0.1) => Acc=0.64815 !!!! \n",
    "### GradientBoosting (n_estimators=25, max_depth=3, learning_rate=0.1) => Acc=0.6495 !!!!\n",
    "## GradientBoosting (n_estimators=50, max_depth=3, learning_rate=0.1) => Acc=0.6506 !!!! [BEST !!!] \n",
    "### GradientBoosting (n_estimators=200, max_depth=3, learning_rate=0.1) => Acc= 0.6477 (takes ~10 hrs!)\n",
    "\n",
    "### GradientBoosting (n_estimators=50, max_depth=3, learning_rate=0.1) => Acc=0.6506 (sess w 1,2,4,... hrs) - NO CHANGE\n",
    "\n",
    "### GradientBoosting (n_estimators=50, max_depth=3, learning_rate=0.1) => Acc=0.7010 (training set is comprised ONLY of samples w session info) \n",
    "### GradientBoosting (n_estimators=100, max_depth=3, learning_rate=0.1) => Acc=0.6996 (training set is comprised ONLY of samples w session info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, verbose=1)\n",
    "gbc.fit(X_train, y_train.astype(int))\n",
    "y_pred_gbc = gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print metrics.accuracy_score(y_test.astype(int), y_pred_gbc.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_matrix_gbc = metrics.confusion_matrix(y_test.astype(int), y_pred_gbc.astype(int))\n",
    "print conf_matrix_gbc\n",
    "print sum(sum(conf_matrix_gbc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_proba_test_gbc = gbc.predict_proba(X_test)\n",
    "predict_proba_test_gbc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_indexes = {}\n",
    "for country,index in y_labels.items():\n",
    "    y_indexes[index] = country\n",
    "print y_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    probas = predict_proba_test_gbc[i]\n",
    "    probas_indexed = []\n",
    "    for j in range(12):\n",
    "        probas_indexed.append([y_indexes[j], probas[j]])\n",
    "    probas_indexed.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    print \"item %d: labels: %s\" % (i, probas_indexed[0:5])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_importances = []\n",
    "for i in range(len(X_test.columns)):\n",
    "    #print \"%s: %.3f\" % (X_test.columns[i], abo.feature_importances_[i])\n",
    "    feat_importances.append((X_test.columns[i], gbc.feature_importances_[i]))\n",
    "\n",
    "feat_importances = sorted(feat_importances, key=lambda x: x[1], reverse=True)\n",
    "for pair in feat_importances:\n",
    "    print pair[0], pair[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron  (scikit) -- only available @v.0.18.dev0 as of 2016-01-25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first scale data:\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(X_train)  \n",
    "X_train_scaled = scaler.transform(X_train)  \n",
    "# apply same transformation to test data\n",
    "X_test_scaled = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51670, 770)\n",
      "(22145, 770)\n"
     ]
    }
   ],
   "source": [
    "print X_train_scaled.shape\n",
    "print X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#mlperc = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "mlperc = MLPClassifier(hidden_layer_sizes=(15,), verbose=True, random_state=1)\n",
    "mlperc.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (PyBrain) - first SCALE features!!!!\n",
    "\n",
    "## UNSCALED DATA:\n",
    "### ([6], bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer, 30 epochs: test error= ~%36.00, Acc=0.6260\n",
    "\n",
    "### ([20], bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer, 30 epochs: test error= %35.45, Acc=0.6429\n",
    "### ([20], bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer, 60 epochs: test error= %35.45, Acc=0.6455\n",
    "### \"\"\" epoch:   90   train error: 35.72%   test error: 36.26% Acc=0.6374\n",
    "\n",
    "### ([12], bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer, epoch:   30   train error: 36.75%   test error: 36.46%, Acc=0.6354\n",
    "\n",
    "### ([30], bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer, epoch:   30   train error: 41.24%   test error: 40.88%, Acc=0.5912\n",
    "### Smallest test_error: %35.25\n",
    "#### epoch:   31   train error: 35.54%   test error: 35.47% => Acc=0.6453\n",
    "#### epoch:   42   train error: 35.53%   test error: 35.35% => Acc=0.6465\n",
    "\n",
    "### ([24,24], bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer, , Acc=0.6397 (epoch 30)\n",
    "### Smallest test_error: %34.98 epoch:55 => Acc=0.6502\n",
    "\n",
    "### ([24,24,24], bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer,\n",
    "### Smallest test_error: %35.35 epoch:35/60 => Acc=0.6465\n",
    "\n",
    "### ([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24], bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer, \n",
    "### Smallest epoch:    9   train error: 41.78%   test error: 41.36% => Acc=0.58.64\n",
    "\n",
    "### ([60], bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer, \n",
    "### epoch:   29   train error: 35.52%   test error: 35.40%\n",
    "\n",
    "## SCALED DATA:\n",
    "### ([4], bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer, 50 epochs: test error= ~%XXXX, Acc=0.XXXX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.structure import TanhLayer\n",
    "from pybrain.structure import SoftmaxLayer\n",
    "from pybrain.datasets import ClassificationDataSet\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from pybrain.utilities import percentError\n",
    "\n",
    "trndata = None\n",
    "tstdata = None\n",
    "\n",
    "trndata = ClassificationDataSet(770, 1, nb_classes=12)\n",
    "for i in range(X_train_scaled.shape[0]):\n",
    "    #trndata.addSample(X_train[i:i+1].values.tolist()[0], y_train[i:i+1].values.tolist())\n",
    "    trndata.addSample(X_train_scaled[i:i+1], y_train[i:i+1].values.tolist())\n",
    "    \n",
    "tstdata = ClassificationDataSet(770, 1, nb_classes=12)\n",
    "for i in range(X_test_scaled.shape[0]):\n",
    "    #tstdata.addSample(X_test[i:i+1].values.tolist()[0], y_test[i:i+1].values.tolist())\n",
    "    tstdata.addSample(X_test_scaled[i:i+1], y_test[i:i+1].values.tolist())\n",
    "        \n",
    "#tstdata, trndata = alldata.splitWithProportion( 0.3 )\n",
    "\n",
    "trndata._convertToOneOfMany( )\n",
    "tstdata._convertToOneOfMany( )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error: 0.0212140951562\n",
      "epoch:    1   train error: 34.68%   test error: 35.00%\n",
      "Total error: 0.0195870764483\n",
      "epoch:    2   train error: 32.88%   test error: 33.40%\n",
      "Total error: 0.0191875399933\n",
      "epoch:    3   train error: 32.29%   test error: 32.85%\n",
      "Total error: 0.0189873041933\n",
      "epoch:    4   train error: 31.81%   test error: 32.07%\n",
      "Total error: 0.0188129436134\n",
      "epoch:    5   train error: 31.68%   test error: 32.34%\n",
      "Total error: 0.0187421410748\n",
      "epoch:    6   train error: 31.33%   test error: 31.98%\n",
      "Total error: 0.0186412671662\n",
      "epoch:    7   train error: 32.77%   test error: 33.51%\n",
      "Total error: 0.0185720096949\n",
      "epoch:    8   train error: 30.94%   test error: 31.75%\n",
      "Total error: 0.0185287502989\n",
      "epoch:    9   train error: 32.07%   test error: 32.89%\n",
      "Total error: 0.0184722163939\n",
      "epoch:   10   train error: 30.68%   test error: 31.47%\n",
      "Total error: 0.0184343583148\n",
      "epoch:   11   train error: 30.55%   test error: 31.42%\n",
      "Total error: 0.0184013669993\n",
      "epoch:   12   train error: 30.51%   test error: 31.35%\n",
      "Total error: 0.0183659154976\n",
      "epoch:   13   train error: 30.42%   test error: 31.44%\n",
      "Total error: 0.0183427280585\n",
      "epoch:   14   train error: 30.33%   test error: 31.33%\n",
      "Total error: 0.0183244335885\n",
      "epoch:   15   train error: 30.21%   test error: 31.44%\n",
      "Total error: 0.0182871913659\n",
      "epoch:   16   train error: 32.06%   test error: 33.22%\n",
      "Total error: 0.0182780585849\n",
      "epoch:   17   train error: 30.19%   test error: 31.32%\n",
      "Total error: 0.0182406262453\n",
      "epoch:   18   train error: 30.11%   test error: 31.21%\n",
      "Total error: 0.0182252706963\n",
      "epoch:   19   train error: 30.15%   test error: 31.29%\n",
      "Total error: 0.0182253786895\n",
      "epoch:   20   train error: 30.24%   test error: 31.38%\n",
      "Total error: 0.0181890420497\n",
      "epoch:   21   train error: 30.01%   test error: 31.35%\n",
      "Total error: 0.018180607621\n",
      "epoch:   22   train error: 29.99%   test error: 31.30%\n",
      "Total error: 0.0181775025123\n",
      "epoch:   23   train error: 30.02%   test error: 31.33%\n",
      "Total error: 0.0181553068163\n",
      "epoch:   24   train error: 29.88%   test error: 31.14%\n",
      "Total error: 0.0181446400342\n",
      "epoch:   25   train error: 29.91%   test error: 31.21%\n",
      "Total error: 0.01811946995\n",
      "epoch:   26   train error: 29.90%   test error: 31.20%\n",
      "Total error: 0.0181170682154\n",
      "epoch:   27   train error: 29.79%   test error: 31.19%\n",
      "Total error: 0.0181017414373\n",
      "epoch:   28   train error: 29.79%   test error: 31.13%\n",
      "Total error: 0.0181026834177\n",
      "epoch:   29   train error: 29.79%   test error: 31.09%\n",
      "Total error: 0.0180868592773\n",
      "epoch:   30   train error: 29.77%   test error: 31.16%\n",
      "Total error: 0.0180856115651\n",
      "epoch:   31   train error: 29.84%   test error: 31.10%\n",
      "Total error: 0.0180797255501\n",
      "epoch:   32   train error: 29.85%   test error: 31.22%\n",
      "Total error: 0.0180725179384\n",
      "epoch:   33   train error: 29.86%   test error: 31.25%\n",
      "Total error: 0.0180606379355\n",
      "epoch:   34   train error: 29.73%   test error: 31.29%\n",
      "Total error: 0.0180529243621\n",
      "epoch:   35   train error: 29.66%   test error: 31.18%\n",
      "Total error: 0.0180509611264\n",
      "epoch:   36   train error: 29.77%   test error: 31.29%\n",
      "Total error: 0.0180344368598\n",
      "epoch:   37   train error: 29.66%   test error: 31.10%\n",
      "Total error: 0.0180284062963\n",
      "epoch:   38   train error: 31.61%   test error: 33.07%\n",
      "Total error: 0.0180251472512\n",
      "epoch:   39   train error: 29.60%   test error: 31.07%\n",
      "Total error: 0.0180173598862\n",
      "epoch:   40   train error: 29.79%   test error: 31.28%\n",
      "Total error: 0.0180136878796\n",
      "epoch:   41   train error: 29.71%   test error: 31.23%\n",
      "Total error: 0.0180269829545\n",
      "epoch:   42   train error: 29.68%   test error: 31.18%\n",
      "Total error: 0.0180278319803\n",
      "epoch:   43   train error: 29.72%   test error: 31.16%\n",
      "Total error: 0.0179982611362\n",
      "epoch:   44   train error: 29.54%   test error: 31.19%\n",
      "Total error: 0.0179869769912\n",
      "epoch:   45   train error: 29.62%   test error: 31.10%\n",
      "Total error: 0.0180037804111\n",
      "epoch:   46   train error: 29.54%   test error: 31.12%\n",
      "Total error: 0.0179942991644\n",
      "epoch:   47   train error: 29.51%   test error: 31.10%\n",
      "Total error: 0.017984079601\n",
      "epoch:   48   train error: 29.50%   test error: 31.07%\n",
      "Total error: 0.0179827378047\n",
      "epoch:   49   train error: 29.56%   test error: 31.04%\n",
      "Total error: 0.0179817041147\n",
      "epoch:   50   train error: 29.66%   test error: 31.07%\n"
     ]
    }
   ],
   "source": [
    "#net = buildNetwork(150, 100, 50, 12, 1, bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer)\n",
    "#fnn = buildNetwork( trndata.indim, 20, 20, 20, 20, 20, trndata.outdim, bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer ) # yields %45.62 test error\n",
    "fnn = buildNetwork( trndata.indim, 4, trndata.outdim, bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer)\n",
    "                   \n",
    "#trainer = BackpropTrainer( fnn, dataset=trndata, momentum=0.1, verbose=True, weightdecay=0.01) \n",
    "trainer = BackpropTrainer( fnn, dataset=trndata, verbose=True) \n",
    "\n",
    "for i in range(50):\n",
    "    trainer.trainEpochs( 1 )\n",
    "    trnresult = percentError( trainer.testOnClassData( dataset=trndata ), trndata['class'] )\n",
    "    tstresult = percentError( trainer.testOnClassData( dataset=tstdata ), tstdata['class'] )\n",
    "\n",
    "    print \"epoch: %4d\" % trainer.totalepochs, \\\n",
    "          \"  train error: %5.2f%%\" % trnresult, \\\n",
    "          \"  test error: %5.2f%%\" % tstresult   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print some shapes and vars\n",
    "print \"X_train.shape: %s\" % str(X_train.shape)\n",
    "print \"y_train.shape: %s\" % str(y_train.shape)\n",
    "print \"X_test.shape: %s\" % str(X_test.shape)\n",
    "print \"y_test.shape: %s\" % str(y_test.shape)\n",
    "\n",
    "print \"X_train_scaled.shape: %s\" % str(X_train_scaled.shape)\n",
    "print \"X_test_scaled.shape: %s\" % str(X_test_scaled.shape)\n",
    "\n",
    "print X_train_scaled[0:5]\n",
    "\n",
    "print \"trndata:\\n%s\" % trndata\n",
    "print \"tstdata:\\n%s\" % tstdata\n",
    "\n",
    "print \"model:\\n%s\" % fnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_fnn = trainer.testOnClassData(dataset=tstdata)\n",
    "conf_matrix_fnn = metrics.confusion_matrix(tstdata[\"class\"], y_pred_fnn)\n",
    "print conf_matrix_fnn\n",
    "print sum(sum(conf_matrix_fnn))\n",
    "metrics.accuracy_score(tstdata[\"class\"], y_pred_fnn)\n",
    "#tstdata[\"class\"].shape\n",
    "#trainer.testOnClassData(dataset=tstdata).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    trainer.trainEpochs( 1 )\n",
    "    trnresult = percentError( trainer.testOnClassData(),\n",
    "                              trndata['class'] )\n",
    "    tstresult = percentError( trainer.testOnClassData(\n",
    "           dataset=tstdata ), tstdata['class'] )\n",
    "\n",
    "    print \"epoch: %4d\" % trainer.totalepochs, \\\n",
    "          \"  train error: %5.2f%%\" % trnresult, \\\n",
    "          \"  test error: %5.2f%%\" % tstresult    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
