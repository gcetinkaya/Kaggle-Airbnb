{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"./data/X.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213466, 150)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_countries = pd.read_csv(\"./data/countries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country_destination  lat_destination  lng_destination  distance_km  \\\n",
      "0                  AU       -26.853388       133.275160   15297.7440   \n",
      "1                  CA        62.393303       -96.818146    2828.1333   \n",
      "2                  DE        51.165707        10.452764    7879.5680   \n",
      "3                  ES        39.896027        -2.487694    7730.7240   \n",
      "4                  FR        46.232193         2.209667    7682.9450   \n",
      "5                  GB        54.633220        -3.432277    6883.6590   \n",
      "6                  IT        41.873990        12.564167    8636.6310   \n",
      "7                  NL        52.133057         5.295250    7524.3203   \n",
      "8                  PT        39.553444        -7.839319    7355.2534   \n",
      "9                  US        36.966427       -95.844030       0.0000   \n",
      "\n",
      "   destination_km2 destination_language   language_levenshtein_distance  \n",
      "0          7741220                   eng                           0.00  \n",
      "1          9984670                   eng                           0.00  \n",
      "2           357022                   deu                          72.61  \n",
      "3           505370                   spa                          92.25  \n",
      "4           643801                   fra                          92.06  \n",
      "5           243610                   eng                           0.00  \n",
      "6           301340                   ita                          89.40  \n",
      "7            41543                   nld                          63.22  \n",
      "8            92090                   por                          95.45  \n",
      "9          9826675                   eng                           0.00  \n"
     ]
    }
   ],
   "source": [
    "print data_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init X_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_countries = pd.core.frame.DataFrame()\n",
    "X_countries[\"country_destination\"] = data_countries[\"country_destination\"]\n",
    "X_countries[\"language_levenshtein_distance\"] = data_countries[\"language_levenshtein_distance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize distance_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create x, where x the 'scores' column's values as floats\n",
    "x = data_countries['distance_km'].values.astype(float)\n",
    "\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "# Run the normalizer on the dataframe\n",
    "X_countries[\"distance_km\"] = x_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize destination_km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create x, where x the 'scores' column's values as floats\n",
    "x = data_countries['destination_km2'].values.astype(float)\n",
    "\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "# Run the normalizer on the dataframe\n",
    "X_countries[\"destination_km2\"] = x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize language_levenshtein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "x = data_countries['language_levenshtein_distance'].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "X_countries[\"language_levenshtein_distance\"] = x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_destination</th>\n",
       "      <th>language_levenshtein_distance</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>destination_km2</th>\n",
       "      <th>population_in_thousands</th>\n",
       "      <th>population_in_thousands_norm</th>\n",
       "      <th>avg_monthly_search_volumes</th>\n",
       "      <th>avg_monthly_search_volumes_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AU</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774372</td>\n",
       "      <td>23923</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>301000</td>\n",
       "      <td>0.489744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184873</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35871</td>\n",
       "      <td>0.080318</td>\n",
       "      <td>246000</td>\n",
       "      <td>0.348718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>0.760712</td>\n",
       "      <td>0.515080</td>\n",
       "      <td>0.031728</td>\n",
       "      <td>82562</td>\n",
       "      <td>0.228769</td>\n",
       "      <td>246000</td>\n",
       "      <td>0.348718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ES</td>\n",
       "      <td>0.966475</td>\n",
       "      <td>0.505351</td>\n",
       "      <td>0.046648</td>\n",
       "      <td>47203</td>\n",
       "      <td>0.116348</td>\n",
       "      <td>165000</td>\n",
       "      <td>0.141026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FR</td>\n",
       "      <td>0.964484</td>\n",
       "      <td>0.502227</td>\n",
       "      <td>0.060570</td>\n",
       "      <td>64983</td>\n",
       "      <td>0.172878</td>\n",
       "      <td>201000</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GB</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449979</td>\n",
       "      <td>0.020322</td>\n",
       "      <td>63840</td>\n",
       "      <td>0.169244</td>\n",
       "      <td>150000</td>\n",
       "      <td>0.102564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IT</td>\n",
       "      <td>0.936616</td>\n",
       "      <td>0.564569</td>\n",
       "      <td>0.026128</td>\n",
       "      <td>61145</td>\n",
       "      <td>0.160675</td>\n",
       "      <td>201000</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NL</td>\n",
       "      <td>0.662336</td>\n",
       "      <td>0.491858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16848</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PT</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.480806</td>\n",
       "      <td>0.005084</td>\n",
       "      <td>10609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>US</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984110</td>\n",
       "      <td>325132</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_destination  language_levenshtein_distance  distance_km  \\\n",
       "0                  AU                       0.000000     1.000000   \n",
       "1                  CA                       0.000000     0.184873   \n",
       "2                  DE                       0.760712     0.515080   \n",
       "3                  ES                       0.966475     0.505351   \n",
       "4                  FR                       0.964484     0.502227   \n",
       "5                  GB                       0.000000     0.449979   \n",
       "6                  IT                       0.936616     0.564569   \n",
       "7                  NL                       0.662336     0.491858   \n",
       "8                  PT                       1.000000     0.480806   \n",
       "9                  US                       0.000000     0.000000   \n",
       "\n",
       "   destination_km2  population_in_thousands  population_in_thousands_norm  \\\n",
       "0         0.774372                    23923                      0.042331   \n",
       "1         1.000000                    35871                      0.080318   \n",
       "2         0.031728                    82562                      0.228769   \n",
       "3         0.046648                    47203                      0.116348   \n",
       "4         0.060570                    64983                      0.172878   \n",
       "5         0.020322                    63840                      0.169244   \n",
       "6         0.026128                    61145                      0.160675   \n",
       "7         0.000000                    16848                      0.019836   \n",
       "8         0.005084                    10609                      0.000000   \n",
       "9         0.984110                   325132                      1.000000   \n",
       "\n",
       "   avg_monthly_search_volumes  avg_monthly_search_volumes_norm  \n",
       "0                      301000                         0.489744  \n",
       "1                      246000                         0.348718  \n",
       "2                      246000                         0.348718  \n",
       "3                      165000                         0.141026  \n",
       "4                      201000                         0.233333  \n",
       "5                      150000                         0.102564  \n",
       "6                      201000                         0.233333  \n",
       "7                      110000                         0.000000  \n",
       "8                      110000                         0.000000  \n",
       "9                      500000                         1.000000  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_age_gender_bkts = pd.read_csv(\"./data/age_gender_bkts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set population_in_thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: AU, popul: 23923\n",
      "country: CA, popul: 35871\n",
      "country: DE, popul: 82562\n",
      "country: ES, popul: 47203\n",
      "country: FR, popul: 64983\n",
      "country: GB, popul: 63840\n",
      "country: IT, popul: 61145\n",
      "country: NL, popul: 16848\n",
      "country: PT, popul: 10609\n",
      "country: US, popul: 325132\n"
     ]
    }
   ],
   "source": [
    "pops = []\n",
    "for country in X_countries.country_destination.values:\n",
    "    print \"country: %s, popul: %d\" % (country, sum(data_age_gender_bkts[data_age_gender_bkts.country_destination == country].population_in_thousands.values))\n",
    "    pops.append(sum(data_age_gender_bkts[data_age_gender_bkts.country_destination == country].population_in_thousands.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_countries[\"population_in_thousands\"] = pops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set population_in_thousands_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = X_countries['population_in_thousands'].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x.reshape(-1,1))\n",
    "X_countries[\"population_in_thousands_norm\"] = x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set search volume (from US) for each country by Google keyword tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    AU\n",
       "1    CA\n",
       "2    DE\n",
       "3    ES\n",
       "4    FR\n",
       "5    GB\n",
       "6    IT\n",
       "7    NL\n",
       "8    PT\n",
       "9    US\n",
       "Name: country_destination, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_countries.country_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_monthly_search_volumes = [301000, 246000, 246000, 165000, 201000, 110000+40000, 201000, 110000, 110000, 500000]\n",
    "#                             AU      CA      DE      ES      FR      GB            IT      NL      PT      US\n",
    "X_countries[\"avg_monthly_search_volumes\"] = avg_monthly_search_volumes\n",
    "\n",
    "x = X_countries['avg_monthly_search_volumes'].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x.reshape(-1,1))\n",
    "X_countries[\"avg_monthly_search_volumes_norm\"] = x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set country attributes for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 150)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to test:\n",
    "#X_test = X[:100]\n",
    "#X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for country in X_countries.country_destination.values:\n",
    "    #print \"country: %s, language_levenshtein_distance: %s\" % (country, [X_countries[X_countries.country_destination==country].language_levenshtein_distance.values])\n",
    "    X[\"language_levenshtein_distance_%s\"%country] = np.array([X_countries[X_countries.country_destination==country].language_levenshtein_distance.values] * len(X))\n",
    "    X[\"distance_km_%s\"%country] = np.array([X_countries[X_countries.country_destination==country].distance_km.values] * len(X))\n",
    "    X[\"destination_km2_%s\"%country] = np.array([X_countries[X_countries.country_destination==country].destination_km2.values] * len(X))\n",
    "    X[\"population_in_thousands_norm_%s\"%country] = np.array([X_countries[X_countries.country_destination==country].population_in_thousands_norm.values] * len(X))\n",
    "    X[\"avg_monthly_search_volumes_norm_%s\"%country] = np.array([X_countries[X_countries.country_destination==country].avg_monthly_search_volumes_norm.values] * len(X))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213466, 210)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: AU, age:100+, pop:5\n",
      "country: CA, age:100+, pop:8\n",
      "country: DE, age:100+, pop:17\n",
      "country: ES, age:100+, pop:12\n",
      "country: FR, age:100+, pop:22\n",
      "country: GB, age:100+, pop:16\n",
      "country: IT, age:100+, pop:18\n",
      "country: NL, age:100+, pop:2\n",
      "country: PT, age:100+, pop:1\n",
      "country: US, age:100+, pop:74\n"
     ]
    }
   ],
   "source": [
    "## TEST\n",
    "#for country in X_countries.country_destination.unique():\n",
    "#    print \"country: %s, age:%s, pop:%d\" % (country, \"100+\", sum(data_age_gender_bkts[(data_age_gender_bkts.age_bucket==\"100+\") & (data_age_gender_bkts.country_destination==country)].population_in_thousands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set age_bucket_perc_XX attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x_t = X.loc[:10, [\"age\", \"\"]]\n",
    "#x_t[\"age_group\"] = [None] * (len(x_t))\n",
    "#x_t.loc[0, \"age\"] = 55\n",
    "#print \"initial x_t:\\n\",x_t\n",
    "\n",
    "def set_age_gender(row):\n",
    "    if row.age == 0:\n",
    "        #print \"age is zero, returning..\"\n",
    "        return 0\n",
    "    if row.age > 99:\n",
    "        age_bucket = \"100+\"\n",
    "    else:\n",
    "        age_bucket = \"%d-%d\" % (row.age - row.age%5, row.age + (4-row.age%5))\n",
    "    #row.age_group = \"%d-%d\" % (row.age - row.age%5, row.age + (4-row.age%5))\n",
    "    #print \"age is %d, setting it to %s\" % (row.age, row.age_group)\n",
    "    #age_bucket_pop = sum(data_age_gender_bkts[(data_age_gender_bkts.age_bucket==age_bucket) & (data_age_gender_bkts.country_destination==country)].population_in_thousands)\n",
    "    return sum(data_age_gender_bkts[(data_age_gender_bkts.age_bucket==age_bucket) & (data_age_gender_bkts.country_destination==country)].population_in_thousands)\n",
    "    #country_pop = X_countries[X_countries.country_destination==country].population_in_thousands\n",
    "    #return age_bucket_pop/float(country_pop)\n",
    "    #return age_bucket_pop\n",
    "\n",
    "for country in X_countries.country_destination.unique():\n",
    "    country_pop = X_countries[X_countries.country_destination==country].population_in_thousands\n",
    "    X[\"age_bucket_perc_%s\"%country] = X.apply(set_age_gender, axis=1)/float(country_pop)\n",
    "\n",
    "#print \"processed x_t:\\n\",x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) split to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149426, 210)\n",
      "(149426,)\n",
      "(64040, 210)\n",
      "(64040,)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST new attrs's relevance to perf w ADABoosting (n_estimators=100): 0.6329 (no change)\n",
    "## TEST new attrs's relevance to perf w ADABoosting (n_estimators=120): 0.6331 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "abo = AdaBoostClassifier(n_estimators=120)\n",
    "abo.fit(X_train, y_train.astype(int))\n",
    "y_pred_abo = abo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.633104309806\n"
     ]
    }
   ],
   "source": [
    "print metrics.accuracy_score(y_test.astype(int), y_pred_abo.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32141  5168     0     0     0     0     0     0     0     0     0     0]\n",
      " [10387  8403     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 1754  1231     0     0     0     0     0     0     0     0     0     0]\n",
      " [  868   627     0     0     0     0     0     0     0     0     0     0]\n",
      " [  234   204     0     0     0     0     0     0     0     0     0     0]\n",
      " [  391   324     0     0     0     0     0     0     0     0     0     0]\n",
      " [  394   282     0     0     0     0     0     0     0     0     0     0]\n",
      " [  506   372     0     0     0     0     0     0     0     0     0     0]\n",
      " [   44    21     0     0     0     0     0     0     0     0     0     0]\n",
      " [  124   108     0     0     0     0     0     0     0     0     0     0]\n",
      " [  158   135     0     0     0     0     0     0     0     0     0     0]\n",
      " [   76    88     0     0     0     0     0     0     0     0     0     0]]\n",
      "64040\n"
     ]
    }
   ],
   "source": [
    "conf_matrix_abo = metrics.confusion_matrix(y_test.astype(int), y_pred_abo.astype(int))\n",
    "print conf_matrix_abo\n",
    "print sum(sum(conf_matrix_abo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age 0.17\n",
      "month_acct_created 0.09\n",
      "gender_MALE 0.02\n",
      "signup_flow_23 0.02\n",
      "aff_chnl_other 0.02\n",
      "first_aff_tracked_tracked_other 0.02\n",
      "gender__unknown_ 0.01\n",
      "gender_OTHER 0.01\n",
      "signup_basic 0.01\n",
      "signup_google 0.01\n",
      "signup_flow_0 0.01\n",
      "signup_flow_3 0.01\n",
      "signup_flow_2 0.01\n",
      "signup_flow_1 0.01\n",
      "signup_flow_8 0.01\n",
      "signup_flow_6 0.01\n",
      "signup_flow_12 0.01\n",
      "signup_flow_16 0.01\n",
      "signup_flow_15 0.01\n",
      "signup_flow_21 0.01\n",
      "language_fr 0.01\n",
      "language_de 0.01\n",
      "language_es 0.01\n",
      "language_it 0.01\n",
      "language_pt 0.01\n",
      "language_zh 0.01\n",
      "language_ko 0.01\n",
      "language_ja 0.01\n",
      "language_ru 0.01\n",
      "language_pl 0.01\n",
      "language_el 0.01\n",
      "language_nl 0.01\n",
      "language_da 0.01\n",
      "aff_chnl_seo 0.01\n",
      "aff_chnl_sem_non_brand 0.01\n",
      "aff_chnl_content 0.01\n",
      "aff_chnl_sem_brand 0.01\n",
      "aff_chnl_remarketing 0.01\n",
      "aff_prov_craigslist 0.01\n",
      "aff_prov_facebook 0.01\n",
      "aff_prov_vast 0.01\n",
      "aff_prov_bing 0.01\n",
      "aff_prov_meetup 0.01\n",
      "aff_prov_facebook_open_graph 0.01\n",
      "aff_prov_email_marketing 0.01\n",
      "aff_prov_yahoo 0.01\n",
      "aff_prov_padmapper 0.01\n",
      "aff_prov_gsp 0.01\n",
      "first_aff_tracked_untracked 0.01\n",
      "first_aff_tracked_linked 0.01\n",
      "first_aff_tracked_product 0.01\n",
      "signup_app_Web 0.01\n",
      "signup_app_Moweb 0.01\n",
      "signup_app_iOS 0.01\n",
      "signup_app_Android 0.01\n",
      "first_dev_Mac_Desktop 0.01\n",
      "first_dev_Windows_Desktop 0.01\n",
      "first_dev_iPhone 0.01\n",
      "first_dev_Other_Unknown 0.01\n",
      "first_dev_Desktop__Other_ 0.01\n",
      "first_dev_Android_Tablet 0.01\n",
      "first_dev_iPad 0.01\n",
      "first_dev_Android_Phone 0.01\n",
      "first_browser_Chrome 0.01\n",
      "first_browser_IE 0.01\n",
      "first_browser_Firefox 0.01\n",
      "first_browser_Safari 0.01\n",
      "first_browser_Mobile_Safari 0.01\n",
      "first_browser_Chrome_Mobile 0.01\n",
      "first_browser_Opera 0.01\n",
      "first_browser_Apple_Mail 0.01\n",
      "first_browser_BlackBerry_Browser 0.01\n",
      "gender_FEMALE 0.0\n",
      "signup_facebook 0.0\n",
      "signup_flow_24 0.0\n",
      "signup_flow_5 0.0\n",
      "signup_flow_10 0.0\n",
      "signup_flow_25 0.0\n",
      "signup_flow_4 0.0\n",
      "signup_flow_20 0.0\n",
      "language_en 0.0\n",
      "language_sv 0.0\n",
      "language_hu 0.0\n",
      "language_id 0.0\n",
      "language_fi 0.0\n",
      "language_no 0.0\n",
      "language_tr 0.0\n",
      "language_th 0.0\n",
      "language_cs 0.0\n",
      "language_hr 0.0\n",
      "language_ca 0.0\n",
      "language_is 0.0\n",
      "aff_chnl_direct 0.0\n",
      "aff_chnl_api 0.0\n",
      "aff_prov_direct 0.0\n",
      "aff_prov_google 0.0\n",
      "aff_prov_other 0.0\n",
      "aff_prov_wayn 0.0\n",
      "aff_prov_naver 0.0\n",
      "aff_prov_baidu 0.0\n",
      "aff_prov_yandex 0.0\n",
      "aff_prov_daum 0.0\n",
      "first_aff_tracked_omg 0.0\n",
      "first_aff_tracked_nan 0.0\n",
      "first_aff_tracked_marketing 0.0\n",
      "first_aff_tracked_local_ops 0.0\n",
      "first_dev_SmartPhone__Other_ 0.0\n",
      "first_browser__unknown_ 0.0\n",
      "first_browser_RockMelt 0.0\n",
      "first_browser_Chromium 0.0\n",
      "first_browser_Android_Browser 0.0\n",
      "first_browser_AOL_Explorer 0.0\n",
      "first_browser_Palm_Pre_web_browser 0.0\n",
      "first_browser_Mobile_Firefox 0.0\n",
      "first_browser_TenFourFox 0.0\n",
      "first_browser_IE_Mobile 0.0\n",
      "first_browser_Silk 0.0\n",
      "first_browser_Camino 0.0\n",
      "first_browser_Arora 0.0\n",
      "first_browser_SeaMonkey 0.0\n",
      "first_browser_Iron 0.0\n",
      "first_browser_Sogou_Explorer 0.0\n",
      "first_browser_IceWeasel 0.0\n",
      "first_browser_Opera_Mini 0.0\n",
      "first_browser_SiteKiosk 0.0\n",
      "first_browser_Maxthon 0.0\n",
      "first_browser_Kindle_Browser 0.0\n",
      "first_browser_CoolNovo 0.0\n",
      "first_browser_Conkeror 0.0\n",
      "first_browser_wOSBrowser 0.0\n",
      "first_browser_Google_Earth 0.0\n",
      "first_browser_Crazy_Browser 0.0\n",
      "first_browser_Mozilla 0.0\n",
      "first_browser_OmniWeb 0.0\n",
      "first_browser_PS_Vita_browser 0.0\n",
      "first_browser_NetNewsWire 0.0\n",
      "first_browser_CometBird 0.0\n",
      "first_browser_Comodo_Dragon 0.0\n",
      "first_browser_Flock 0.0\n",
      "first_browser_Pale_Moon 0.0\n",
      "first_browser_Avant_Browser 0.0\n",
      "first_browser_Opera_Mobile 0.0\n",
      "first_browser_Yandex.Browser 0.0\n",
      "first_browser_TheWorld_Browser 0.0\n",
      "first_browser_SlimBrowser 0.0\n",
      "first_browser_Epic 0.0\n",
      "first_browser_Stainless 0.0\n",
      "first_browser_Googlebot 0.0\n",
      "first_browser_Outlook_2007 0.0\n",
      "first_browser_IceDragon 0.0\n",
      "language_levenshtein_distance_AU 0.0\n",
      "language_levenshtein_distance_CA 0.0\n",
      "language_levenshtein_distance_DE 0.0\n",
      "language_levenshtein_distance_ES 0.0\n",
      "language_levenshtein_distance_FR 0.0\n",
      "language_levenshtein_distance_GB 0.0\n",
      "language_levenshtein_distance_IT 0.0\n",
      "language_levenshtein_distance_NL 0.0\n",
      "language_levenshtein_distance_PT 0.0\n",
      "language_levenshtein_distance_US 0.0\n",
      "distance_km_AU 0.0\n",
      "destination_km2_AU 0.0\n",
      "population_in_thousands_norm_AU 0.0\n",
      "distance_km_CA 0.0\n",
      "destination_km2_CA 0.0\n",
      "population_in_thousands_norm_CA 0.0\n",
      "distance_km_DE 0.0\n",
      "destination_km2_DE 0.0\n",
      "population_in_thousands_norm_DE 0.0\n",
      "distance_km_ES 0.0\n",
      "destination_km2_ES 0.0\n",
      "population_in_thousands_norm_ES 0.0\n",
      "distance_km_FR 0.0\n",
      "destination_km2_FR 0.0\n",
      "population_in_thousands_norm_FR 0.0\n",
      "distance_km_GB 0.0\n",
      "destination_km2_GB 0.0\n",
      "population_in_thousands_norm_GB 0.0\n",
      "distance_km_IT 0.0\n",
      "destination_km2_IT 0.0\n",
      "population_in_thousands_norm_IT 0.0\n",
      "distance_km_NL 0.0\n",
      "destination_km2_NL 0.0\n",
      "population_in_thousands_norm_NL 0.0\n",
      "distance_km_PT 0.0\n",
      "destination_km2_PT 0.0\n",
      "population_in_thousands_norm_PT 0.0\n",
      "distance_km_US 0.0\n",
      "destination_km2_US 0.0\n",
      "population_in_thousands_norm_US 0.0\n",
      "avg_monthly_search_volumes_norm_AU 0.0\n",
      "avg_monthly_search_volumes_norm_CA 0.0\n",
      "avg_monthly_search_volumes_norm_DE 0.0\n",
      "avg_monthly_search_volumes_norm_ES 0.0\n",
      "avg_monthly_search_volumes_norm_FR 0.0\n",
      "avg_monthly_search_volumes_norm_GB 0.0\n",
      "avg_monthly_search_volumes_norm_IT 0.0\n",
      "avg_monthly_search_volumes_norm_NL 0.0\n",
      "avg_monthly_search_volumes_norm_PT 0.0\n",
      "avg_monthly_search_volumes_norm_US 0.0\n"
     ]
    }
   ],
   "source": [
    "feat_importances = []\n",
    "for i in range(len(X_test.columns)):\n",
    "    #print \"%s: %.3f\" % (X_test.columns[i], abo.feature_importances_[i])\n",
    "    feat_importances.append((X_test.columns[i], abo.feature_importances_[i]))\n",
    "\n",
    "feat_importances = sorted(feat_importances, key=lambda x: x[1], reverse=True)\n",
    "for pair in feat_importances:\n",
    "    print pair[0], pair[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST new attr's perf w XGBoosting(max_depth=2): 0.6368 (no change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.369878\ttrain-merror:0.370612\n",
      "[1]\teval-merror:0.626390\ttrain-merror:0.623339\n",
      "[2]\teval-merror:0.368723\ttrain-merror:0.369521\n",
      "[3]\teval-merror:0.368239\ttrain-merror:0.368999\n",
      "[4]\teval-merror:0.367302\ttrain-merror:0.368283\n",
      "[5]\teval-merror:0.367473\ttrain-merror:0.367928\n",
      "[6]\teval-merror:0.367364\ttrain-merror:0.367386\n",
      "[7]\teval-merror:0.367146\ttrain-merror:0.367352\n",
      "[8]\teval-merror:0.366084\ttrain-merror:0.366937\n",
      "[9]\teval-merror:0.366240\ttrain-merror:0.366817\n",
      "[10]\teval-merror:0.366099\ttrain-merror:0.366703\n",
      "[11]\teval-merror:0.365896\ttrain-merror:0.366382\n",
      "[12]\teval-merror:0.365912\ttrain-merror:0.365994\n",
      "[13]\teval-merror:0.366099\ttrain-merror:0.365873\n",
      "[14]\teval-merror:0.365662\ttrain-merror:0.365753\n",
      "[15]\teval-merror:0.365272\ttrain-merror:0.365572\n",
      "[16]\teval-merror:0.365475\ttrain-merror:0.365766\n",
      "[17]\teval-merror:0.365334\ttrain-merror:0.365800\n",
      "[18]\teval-merror:0.365522\ttrain-merror:0.365753\n",
      "[19]\teval-merror:0.365709\ttrain-merror:0.365425\n",
      "[20]\teval-merror:0.365600\ttrain-merror:0.365505\n",
      "[21]\teval-merror:0.365490\ttrain-merror:0.365499\n",
      "[22]\teval-merror:0.365693\ttrain-merror:0.365479\n",
      "[23]\teval-merror:0.365693\ttrain-merror:0.365392\n",
      "[24]\teval-merror:0.365693\ttrain-merror:0.365392\n",
      "[25]\teval-merror:0.365194\ttrain-merror:0.365238\n",
      "[26]\teval-merror:0.364725\ttrain-merror:0.364910\n",
      "[27]\teval-merror:0.364772\ttrain-merror:0.364843\n",
      "[28]\teval-merror:0.364725\ttrain-merror:0.364823\n",
      "[29]\teval-merror:0.364881\ttrain-merror:0.364756\n",
      "[30]\teval-merror:0.364913\ttrain-merror:0.364662\n",
      "[31]\teval-merror:0.364959\ttrain-merror:0.364528\n",
      "[32]\teval-merror:0.364928\ttrain-merror:0.364448\n",
      "[33]\teval-merror:0.364663\ttrain-merror:0.364401\n",
      "[34]\teval-merror:0.364678\ttrain-merror:0.364428\n",
      "[35]\teval-merror:0.364756\ttrain-merror:0.364468\n",
      "[36]\teval-merror:0.364788\ttrain-merror:0.364435\n",
      "[37]\teval-merror:0.364788\ttrain-merror:0.364394\n",
      "[38]\teval-merror:0.364678\ttrain-merror:0.364401\n",
      "[39]\teval-merror:0.364866\ttrain-merror:0.364314\n",
      "[40]\teval-merror:0.365100\ttrain-merror:0.364414\n",
      "[41]\teval-merror:0.364991\ttrain-merror:0.364368\n",
      "[42]\teval-merror:0.364866\ttrain-merror:0.364301\n",
      "[43]\teval-merror:0.364913\ttrain-merror:0.364334\n",
      "[44]\teval-merror:0.364897\ttrain-merror:0.364174\n",
      "[45]\teval-merror:0.364913\ttrain-merror:0.364287\n",
      "[46]\teval-merror:0.364522\ttrain-merror:0.364254\n",
      "[47]\teval-merror:0.364725\ttrain-merror:0.364200\n",
      "[48]\teval-merror:0.364741\ttrain-merror:0.364354\n",
      "[49]\teval-merror:0.364631\ttrain-merror:0.364267\n",
      "[50]\teval-merror:0.364678\ttrain-merror:0.364220\n",
      "[51]\teval-merror:0.364694\ttrain-merror:0.364247\n",
      "[52]\teval-merror:0.364647\ttrain-merror:0.364187\n",
      "[53]\teval-merror:0.364803\ttrain-merror:0.364194\n",
      "[54]\teval-merror:0.364788\ttrain-merror:0.364174\n",
      "[55]\teval-merror:0.364788\ttrain-merror:0.364167\n",
      "[56]\teval-merror:0.364850\ttrain-merror:0.364127\n",
      "[57]\teval-merror:0.364944\ttrain-merror:0.364133\n",
      "[58]\teval-merror:0.365006\ttrain-merror:0.364006\n",
      "[59]\teval-merror:0.364803\ttrain-merror:0.363899\n",
      "[60]\teval-merror:0.364631\ttrain-merror:0.363792\n",
      "[61]\teval-merror:0.364553\ttrain-merror:0.363759\n",
      "[62]\teval-merror:0.364366\ttrain-merror:0.363806\n",
      "[63]\teval-merror:0.364350\ttrain-merror:0.363799\n",
      "[64]\teval-merror:0.364569\ttrain-merror:0.363832\n",
      "[65]\teval-merror:0.364616\ttrain-merror:0.363846\n",
      "[66]\teval-merror:0.364272\ttrain-merror:0.364033\n",
      "[67]\teval-merror:0.364257\ttrain-merror:0.364080\n",
      "[68]\teval-merror:0.364257\ttrain-merror:0.364066\n",
      "[69]\teval-merror:0.364304\ttrain-merror:0.364087\n",
      "[70]\teval-merror:0.364272\ttrain-merror:0.364087\n",
      "[71]\teval-merror:0.364257\ttrain-merror:0.364180\n",
      "[72]\teval-merror:0.364475\ttrain-merror:0.364167\n",
      "[73]\teval-merror:0.364444\ttrain-merror:0.363933\n",
      "[74]\teval-merror:0.364382\ttrain-merror:0.364100\n",
      "[75]\teval-merror:0.364210\ttrain-merror:0.364080\n",
      "[76]\teval-merror:0.364585\ttrain-merror:0.363792\n",
      "[77]\teval-merror:0.364054\ttrain-merror:0.363779\n",
      "[78]\teval-merror:0.364194\ttrain-merror:0.363819\n",
      "[79]\teval-merror:0.364319\ttrain-merror:0.363852\n",
      "[80]\teval-merror:0.364304\ttrain-merror:0.363852\n",
      "[81]\teval-merror:0.364304\ttrain-merror:0.363906\n",
      "[82]\teval-merror:0.364241\ttrain-merror:0.363993\n",
      "[83]\teval-merror:0.364272\ttrain-merror:0.363959\n",
      "[84]\teval-merror:0.364241\ttrain-merror:0.363806\n",
      "[85]\teval-merror:0.364241\ttrain-merror:0.363799\n",
      "[86]\teval-merror:0.364179\ttrain-merror:0.363745\n",
      "[87]\teval-merror:0.364194\ttrain-merror:0.363732\n",
      "[88]\teval-merror:0.364147\ttrain-merror:0.363705\n",
      "[89]\teval-merror:0.364163\ttrain-merror:0.363698\n",
      "[90]\teval-merror:0.364194\ttrain-merror:0.363692\n",
      "[91]\teval-merror:0.364085\ttrain-merror:0.363739\n",
      "[92]\teval-merror:0.364163\ttrain-merror:0.363739\n",
      "[93]\teval-merror:0.364054\ttrain-merror:0.363605\n",
      "[94]\teval-merror:0.363929\ttrain-merror:0.363598\n",
      "[95]\teval-merror:0.364304\ttrain-merror:0.363692\n",
      "[96]\teval-merror:0.364101\ttrain-merror:0.363618\n",
      "[97]\teval-merror:0.364038\ttrain-merror:0.363364\n",
      "[98]\teval-merror:0.364007\ttrain-merror:0.363377\n",
      "[99]\teval-merror:0.364147\ttrain-merror:0.363518\n",
      "[100]\teval-merror:0.364022\ttrain-merror:0.363297\n",
      "[101]\teval-merror:0.363960\ttrain-merror:0.363250\n",
      "[102]\teval-merror:0.363866\ttrain-merror:0.363237\n",
      "[103]\teval-merror:0.363882\ttrain-merror:0.363049\n",
      "[104]\teval-merror:0.363710\ttrain-merror:0.363183\n",
      "[105]\teval-merror:0.363616\ttrain-merror:0.363089\n",
      "[106]\teval-merror:0.363585\ttrain-merror:0.363103\n",
      "[107]\teval-merror:0.363585\ttrain-merror:0.363203\n",
      "[108]\teval-merror:0.363538\ttrain-merror:0.363136\n",
      "[109]\teval-merror:0.363523\ttrain-merror:0.363103\n",
      "[110]\teval-merror:0.363570\ttrain-merror:0.363089\n",
      "[111]\teval-merror:0.363663\ttrain-merror:0.363130\n",
      "[112]\teval-merror:0.363632\ttrain-merror:0.363109\n",
      "[113]\teval-merror:0.363523\ttrain-merror:0.363029\n",
      "[114]\teval-merror:0.363523\ttrain-merror:0.362969\n",
      "[115]\teval-merror:0.363398\ttrain-merror:0.362909\n",
      "[116]\teval-merror:0.363413\ttrain-merror:0.362882\n",
      "[117]\teval-merror:0.363382\ttrain-merror:0.362889\n",
      "[118]\teval-merror:0.363413\ttrain-merror:0.362869\n",
      "[119]\teval-merror:0.363242\ttrain-merror:0.362755\n",
      "[120]\teval-merror:0.363351\ttrain-merror:0.362701\n",
      "[121]\teval-merror:0.363398\ttrain-merror:0.362708\n",
      "[122]\teval-merror:0.363429\ttrain-merror:0.362755\n",
      "[123]\teval-merror:0.363460\ttrain-merror:0.362755\n",
      "[124]\teval-merror:0.363429\ttrain-merror:0.362721\n",
      "[125]\teval-merror:0.363320\ttrain-merror:0.362708\n",
      "[126]\teval-merror:0.363273\ttrain-merror:0.362775\n",
      "[127]\teval-merror:0.363351\ttrain-merror:0.362735\n",
      "[128]\teval-merror:0.363226\ttrain-merror:0.362708\n",
      "[129]\teval-merror:0.363148\ttrain-merror:0.362715\n",
      "[130]\teval-merror:0.363210\ttrain-merror:0.362782\n",
      "[131]\teval-merror:0.363257\ttrain-merror:0.362768\n",
      "[132]\teval-merror:0.363273\ttrain-merror:0.362621\n",
      "[133]\teval-merror:0.363445\ttrain-merror:0.362561\n",
      "[134]\teval-merror:0.363710\ttrain-merror:0.362480\n",
      "[135]\teval-merror:0.363726\ttrain-merror:0.362427\n",
      "[136]\teval-merror:0.363132\ttrain-merror:0.362193\n",
      "[137]\teval-merror:0.363226\ttrain-merror:0.362186\n",
      "[138]\teval-merror:0.363242\ttrain-merror:0.362132\n",
      "[139]\teval-merror:0.363367\ttrain-merror:0.362219\n",
      "[140]\teval-merror:0.363320\ttrain-merror:0.362219\n",
      "[141]\teval-merror:0.363476\ttrain-merror:0.362059\n",
      "[142]\teval-merror:0.363695\ttrain-merror:0.362092\n",
      "[143]\teval-merror:0.363913\ttrain-merror:0.362206\n",
      "[144]\teval-merror:0.363960\ttrain-merror:0.362106\n",
      "[145]\teval-merror:0.363710\ttrain-merror:0.362052\n",
      "[146]\teval-merror:0.363741\ttrain-merror:0.362045\n",
      "[147]\teval-merror:0.363492\ttrain-merror:0.362032\n",
      "[148]\teval-merror:0.363507\ttrain-merror:0.361952\n",
      "[149]\teval-merror:0.363538\ttrain-merror:0.362092\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#param = {'bst:max_depth':2, 'bst:eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "param = {'bst:max_depth':2, 'bst:eta':1, 'silent':0, 'objective': 'multi:softmax', 'num_class': 12}\n",
    "param['nthread'] = 4\n",
    "#param['eval_metric'] = 'auc'\n",
    "num_round = 150\n",
    "dtrain = xgb.DMatrix( X_train, label=y_train )\n",
    "dtest = xgb.DMatrix( X_test, label=y_test )\n",
    "evallist  = [(dtest,'eval'), (dtrain,'train')]\n",
    "#bst = xgb.train( param.items(), dtrain, num_round, evallist, early_stopping_rounds=10)\n",
    "bst = xgb.train( param.items(), dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_xgb = bst.predict(xgb.DMatrix(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.618566520924\n"
     ]
    }
   ],
   "source": [
    "print metrics.accuracy_score(y_test.astype(int), y_pred_xgb.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31351  5721    77    39    18    26    14    23    11     5    14    10]\n",
      " [ 9788  8743    70    38    18    19    18    32     4    21    27    12]\n",
      " [ 1667  1258    22    10     1     1     5     6     2     3     7     3]\n",
      " [  831   640     6     3     1     1     2     4     0     1     4     2]\n",
      " [  229   205     2     0     0     1     0     0     0     0     0     1]\n",
      " [  374   326     2     2     1     0     2     2     1     3     0     2]\n",
      " [  389   283     1     1     0     0     0     0     0     1     1     0]\n",
      " [  479   381     4     0     1     3     3     3     1     1     1     1]\n",
      " [   42    22     0     1     0     0     0     0     0     0     0     0]\n",
      " [  115   111     1     3     0     0     0     1     0     0     0     1]\n",
      " [  156   134     1     1     0     0     0     0     0     0     1     0]\n",
      " [   88    74     1     1     0     0     0     0     0     0     0     0]]\n",
      "64040\n"
     ]
    }
   ],
   "source": [
    "conf_matrix_xgb = metrics.confusion_matrix(y_test.astype(int), y_pred_xgb.astype(int))\n",
    "print conf_matrix_xgb\n",
    "print sum(sum(conf_matrix_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST new attrs relevance to performance w FNN: 0.6336 (~no change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is indexed by dict:\n",
      "{'FR': 3, 'NL': 9, 'PT': 8, 'CA': 4, 'DE': 10, 'IT': 7, 'US': 1, 'NDF': 0, 'other': 2, 'AU': 11, 'GB': 5, 'ES': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 5, 9, 0])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(\"./data/y.csv\")\n",
    "y_labels = {}\n",
    "i = 0\n",
    "for value in y.country_destination.unique():\n",
    "    if value not in y_labels.keys():\n",
    "        y_labels[value] = i\n",
    "        i += 1\n",
    "    y[y == value] = i-1\n",
    "\n",
    "print \"y is indexed by dict:\\n%s\" % y_labels\n",
    "\n",
    "y = y.country_destination\n",
    "y.ravel().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(213466, 200)\n",
      "(213466,)\n"
     ]
    }
   ],
   "source": [
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error: 0.0222906200351\n",
      "epoch:    1   train error: 43.02%   test error: 42.67%\n",
      "Total error: 0.0218670071775\n",
      "epoch:    2   train error: 37.70%   test error: 37.70%\n",
      "Total error: 0.0217072342001\n",
      "epoch:    3   train error: 38.33%   test error: 38.05%\n",
      "Total error: 0.0216119301224\n",
      "epoch:    4   train error: 37.29%   test error: 37.24%\n",
      "Total error: 0.0215218468419\n",
      "epoch:    5   train error: 39.08%   test error: 38.96%\n",
      "Total error: 0.0214592080769\n",
      "epoch:    6   train error: 37.52%   test error: 37.44%\n",
      "Total error: 0.0214182464879\n",
      "epoch:    7   train error: 37.03%   test error: 36.97%\n",
      "Total error: 0.0214048940655\n",
      "epoch:    8   train error: 36.91%   test error: 36.81%\n",
      "Total error: 0.0213679257671\n",
      "epoch:    9   train error: 39.13%   test error: 39.10%\n",
      "Total error: 0.0213744067891\n",
      "epoch:   10   train error: 37.18%   test error: 37.26%\n",
      "Total error: 0.0213652807028\n",
      "epoch:   11   train error: 37.28%   test error: 37.41%\n",
      "Total error: 0.0213507612538\n",
      "epoch:   12   train error: 37.58%   test error: 37.46%\n",
      "Total error: 0.0213331546853\n",
      "epoch:   13   train error: 36.90%   test error: 36.83%\n",
      "Total error: 0.0213315893575\n",
      "epoch:   14   train error: 38.19%   test error: 38.07%\n",
      "Total error: 0.0213277766003\n",
      "epoch:   15   train error: 37.10%   test error: 36.93%\n",
      "Total error: 0.0213283008489\n",
      "epoch:   16   train error: 37.70%   test error: 37.71%\n",
      "Total error: 0.0212946515268\n",
      "epoch:   17   train error: 36.83%   test error: 36.78%\n",
      "Total error: 0.0212747689174\n",
      "epoch:   18   train error: 38.51%   test error: 38.50%\n",
      "Total error: 0.0212747023605\n",
      "epoch:   19   train error: 39.06%   test error: 38.97%\n",
      "Total error: 0.0213005223729\n",
      "epoch:   20   train error: 37.45%   test error: 37.48%\n",
      "Total error: 0.0212956650086\n",
      "epoch:   21   train error: 38.12%   test error: 38.00%\n",
      "Total error: 0.0212954543694\n",
      "epoch:   22   train error: 36.76%   test error: 36.64%\n",
      "Total error: 0.0212925743343\n",
      "epoch:   23   train error: 38.54%   test error: 38.37%\n",
      "Total error: 0.0212670799765\n",
      "epoch:   24   train error: 36.86%   test error: 36.73%\n",
      "Total error: 0.0212359118583\n",
      "epoch:   25   train error: 36.75%   test error: 36.75%\n",
      "Total error: 0.0212914446824\n",
      "epoch:   26   train error: 36.99%   test error: 36.88%\n",
      "Total error: 0.0212620633613\n",
      "epoch:   27   train error: 37.12%   test error: 37.01%\n",
      "Total error: 0.0212257403894\n",
      "epoch:   28   train error: 37.39%   test error: 37.40%\n",
      "Total error: 0.0211966674771\n",
      "epoch:   29   train error: 37.64%   test error: 37.74%\n",
      "Total error: 0.0212205600403\n",
      "epoch:   30   train error: 36.69%   test error: 36.80%\n"
     ]
    }
   ],
   "source": [
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.structure import TanhLayer\n",
    "from pybrain.structure import SoftmaxLayer\n",
    "from pybrain.datasets import ClassificationDataSet\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from pybrain.utilities import percentError\n",
    "\n",
    "alldata = ClassificationDataSet(190, 1, nb_classes=12)\n",
    "for i in range(X.shape[0]):\n",
    "    alldata.addSample(X[i:i+1].values.tolist()[0], y[i:i+1].values.tolist())\n",
    "\n",
    "tstdata, trndata = alldata.splitWithProportion( 0.3 )\n",
    "\n",
    "trndata._convertToOneOfMany( )\n",
    "tstdata._convertToOneOfMany( )\n",
    "\n",
    "#net = buildNetwork(150, 100, 50, 12, 1, bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer)\n",
    "#fnn = buildNetwork( trndata.indim, 20, 20, 20, 20, 20, trndata.outdim, bias=True, hiddenclass=TanhLayer, outclass=SoftmaxLayer ) # yields %45.62 test error\n",
    "fnn = buildNetwork( trndata.indim, 100, 70, 50, 30, 20, trndata.outdim, outclass=SoftmaxLayer)\n",
    "                   \n",
    "#trainer = BackpropTrainer( fnn, dataset=trndata, momentum=0.1, verbose=True, weightdecay=0.01) \n",
    "trainer = BackpropTrainer( fnn, dataset=trndata, verbose=True) \n",
    "\n",
    "for i in range(30):\n",
    "    trainer.trainEpochs( 1 )\n",
    "    trnresult = percentError( trainer.testOnClassData(),\n",
    "                              trndata['class'] )\n",
    "    tstresult = percentError( trainer.testOnClassData(\n",
    "           dataset=tstdata ), tstdata['class'] )\n",
    "\n",
    "    print \"epoch: %4d\" % trainer.totalepochs, \\\n",
    "          \"  train error: %5.2f%%\" % trnresult, \\\n",
    "          \"  test error: %5.2f%%\" % tstresult    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
